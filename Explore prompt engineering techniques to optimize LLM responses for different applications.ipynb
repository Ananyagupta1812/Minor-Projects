{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhBgNRRD_2h1"
      },
      "source": [
        "# **Explore prompt engineering techniques to optimize LLM responses for different applications.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOe_93ma_2h2"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates various **prompt engineering techniques** to optimize Large Language Model (LLM) responses for different applications. We'll explore 5 key techniques using Microsoft's Phi-3 model:\n",
        "\n",
        "### Learning Objectives:\n",
        "1. **Understand prompt engineering fundamentals** - How different prompting strategies affect model output\n",
        "2. **Implement role assignment** - Guide the model to adopt specific personas or expertise levels\n",
        "3. **Apply chain-of-thought prompting** - Break down complex reasoning into step-by-step processes\n",
        "4. **Use output constraints** - Control response format, length, and structure\n",
        "5. **Leverage few-shot learning** - Provide examples to guide model behavior\n",
        "\n",
        "### Techniques Covered:\n",
        "- **Role Assignment** - Making the AI adopt specific personas\n",
        "- **Chain-of-Thought** - Step-by-step reasoning\n",
        "- **Output Constraints** - Controlling response format\n",
        "- **Hypothetical Scenarios** - Context-based prompting\n",
        "- **Few-Shot Learning** - Learning from examples\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0w7rYkV_2h2"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Before we begin exploring prompt engineering techniques, we need to install the required dependencies:\n",
        "\n",
        "### Required Libraries:\n",
        "- **torch** - PyTorch deep learning framework for tensor operations\n",
        "- **transformers** - Hugging Face library for pre-trained language models\n",
        "- **accelerate** - Library for distributed training and inference optimization\n",
        "- **bitsandbytes** - Enables efficient 4-bit quantization to reduce memory usage\n",
        "\n",
        "### Why These Dependencies?\n",
        "1. **Memory Efficiency**: 4-bit quantization allows us to run large models on consumer hardware\n",
        "2. **Model Access**: Transformers library provides easy access to state-of-the-art models\n",
        "3. **Performance**: Accelerate optimizes inference speed and memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvv6yUdyD0Dt"
      },
      "outputs": [],
      "source": [
        "# Install core dependencies for LLM inference\n",
        "# torch: PyTorch framework for deep learning operations\n",
        "# transformers: Hugging Face library for accessing pre-trained models\n",
        "# accelerate: Optimizes model loading and inference across devices\n",
        "# bitsandbytes: Enables 4-bit quantization for memory-efficient inference\n",
        "!pip install torch transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaOyGS8XEcED"
      },
      "outputs": [],
      "source": [
        "# Update bitsandbytes to the latest version for optimal 4-bit quantization support\n",
        "# The -U flag ensures we get the most recent version with latest optimizations\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrsrWDSY_2h4"
      },
      "source": [
        "## Model Setup and Prompt Engineering Implementation\n",
        "\n",
        "### Model Choice: Microsoft Phi-3-mini-4k-instruct\n",
        "We're using **Phi-3-mini-4k-instruct** because:\n",
        "- **Compact Size**: ~3.8B parameters, suitable for consumer hardware\n",
        "- **Instruction-Tuned**: Optimized for following complex prompts and instructions\n",
        "- **4K Context**: Can handle moderately long conversations and contexts\n",
        "- **High Quality**: Competitive performance despite smaller size\n",
        "\n",
        "### Key Implementation Features:\n",
        "\n",
        "#### **Model Configuration**\n",
        "- **4-bit Quantization**: Reduces memory usage by ~75% while maintaining quality\n",
        "- **Float16 Precision**: Balances speed and accuracy\n",
        "- **Auto Device Mapping**: Automatically distributes model across available GPUs/CPU\n",
        "\n",
        "#### **Inference Function**\n",
        "- **Configurable Generation**: Temperature control for creativity vs consistency\n",
        "- **Token Limits**: Prevents runaway generation with max_new_tokens\n",
        "- **Clean Output**: Removes special tokens for readable responses\n",
        "\n",
        "#### **Prompt Engineering Techniques**\n",
        "Each technique demonstrates a different approach to guide model behavior:\n",
        "\n",
        "1. **Role Assignment** - Assigns expertise and target audience\n",
        "2. **Chain-of-Thought** - Encourages step-by-step reasoning\n",
        "3. **Output Constraints** - Controls format and length precisely\n",
        "4. **Hypothetical Scenarios** - Provides creative context\n",
        "5. **Few-Shot Learning** - Shows examples to guide behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNqUctWjDIGs"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for model loading and inference\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# ===== Model Configuration =====\n",
        "# Specify the pre-trained model to use for our experiments\n",
        "# Phi-3-mini-4k-instruct is chosen for its balance of performance and efficiency\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "print(f\"Loading {model_name} in 4-bit mode...\")\n",
        "\n",
        "# Initialize the tokenizer to convert text to tokens and vice versa\n",
        "# The tokenizer handles text preprocessing and post-processing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model with optimized configuration for memory efficiency\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",          # Automatically distribute model across available devices\n",
        "    torch_dtype=torch.float16,  # Use half-precision to reduce memory usage\n",
        "    load_in_4bit=True          # Enable 4-bit quantization for maximum memory efficiency\n",
        ")\n",
        "\n",
        "def run_inference(prompt):\n",
        "    \"\"\"\n",
        "    Generate LLM response from a given prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input text prompt to generate a response for\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response text (decoded and cleaned)\n",
        "\n",
        "    This function encapsulates the complete inference pipeline:\n",
        "    1. Tokenize the input prompt\n",
        "    2. Generate tokens using the model\n",
        "    3. Decode tokens back to human-readable text\n",
        "    \"\"\"\n",
        "    # Convert the prompt text into tokens that the model can process\n",
        "    # return_tensors=\"pt\" ensures PyTorch tensor format\n",
        "    # .to(model.device) moves tensors to the same device as the model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate new tokens based on the input prompt\n",
        "    outputs = model.generate(\n",
        "        **inputs,                    # Unpack input tensors (input_ids, attention_mask, etc.)\n",
        "        max_new_tokens=200,         # Limit response length to prevent runaway generation\n",
        "        temperature=0.7             # Control randomness: 0.0=deterministic, 1.0=very creative\n",
        "    )\n",
        "\n",
        "    # Convert generated tokens back to human-readable text\n",
        "    # skip_special_tokens=True removes technical tokens like <pad>, <eos>, etc.\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ===== Prompt Engineering Techniques =====\n",
        "# Dictionary containing 5 different prompting strategies with example prompts\n",
        "# Each technique demonstrates a different way to guide model behavior\n",
        "techniques = {\n",
        "    # 1. ROLE ASSIGNMENT: Assign specific expertise and target audience\n",
        "    # This technique makes the AI adopt a particular persona or professional role\n",
        "    \"1. Role Assignment\":\n",
        "        \"You are a professional science teacher. Explain quantum computing to a 10-year-old in 3 short bullet points.\",\n",
        "\n",
        "    # 2. CHAIN-OF-THOUGHT: Encourage step-by-step reasoning\n",
        "    # This helps the model break down complex topics into logical steps\n",
        "    \"2. Step-by-Step (Chain-of-Thought)\":\n",
        "        \"Explain quantum computing step-by-step, then provide a one-sentence summary at the end.\",\n",
        "\n",
        "    # 3. OUTPUT CONSTRAINTS: Specify exact format and length requirements\n",
        "    # This gives precise control over response structure and length\n",
        "    \"3. Output Constraints\":\n",
        "        \"Explain quantum computing in exactly 4 sentences, each under 12 words.\",\n",
        "\n",
        "    # 4. HYPOTHETICAL SCENARIOS: Provide creative context for explanations\n",
        "    # This leverages familiar contexts to make complex topics more relatable\n",
        "    \"4. Hypothetical Scenario\":\n",
        "        \"Imagine you are a sports commentator explaining quantum computing to football fans.\",\n",
        "\n",
        "    # 5. FEW-SHOT LEARNING: Provide examples to guide the desired output style\n",
        "    # This shows the model the pattern of responses we want\n",
        "    \"5. Few-Shot Learning\":\n",
        "        \"Example: Classical computing is like flipping light switches on or off.\\n\"\n",
        "        \"Example: Quantum computing is like dimmer switches that can be on, off, or in between.\\n\"\n",
        "        \"Now give 2 more creative analogies for quantum computing.\"\n",
        "}\n",
        "\n",
        "# ===== Run Experiments =====\n",
        "# Iterate through each technique and demonstrate its effectiveness\n",
        "print(\"Starting Prompt Engineering Experiments...\\n\")\n",
        "\n",
        "for name, prompt in techniques.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Response:\", run_inference(prompt))\n",
        "    print(\"-\" * 80)  # Visual separator between experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN8ld-eQDYFk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}