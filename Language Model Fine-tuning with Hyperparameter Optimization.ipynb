{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9706f5b0-75fc-418f-a54a-f03c07a9c083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate ray[tune] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3b255-b353-4cfe-9cdc-4d5dd94ee98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 05:28:38,649\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67059712 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-11 05:28:39,776\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-07-11 05:28:40,383\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-11 05:29:09</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:28.89        </td></tr>\n",
       "<tr><td>Memory:      </td><td>66.0/1007.7 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: -4.114792227745056<br>Logical resource usage: 1.0/2 CPUs, 0.5/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_with_tune_e7281_00000</td><td>TERMINATED</td><td>10.0.62.11:175877</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">1.26073e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         19.1564</td><td style=\"text-align: right;\">4.39772</td></tr>\n",
       "<tr><td>train_with_tune_e7281_00001</td><td>TERMINATED</td><td>10.0.62.11:175878</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">9.26354e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.7124</td><td style=\"text-align: right;\">4.02048</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m /usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m 🔧 Starting trial with config: {'batch_size': 2, 'lr': 1.2607329607942673e-05, 'epochs': 1}\n",
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m 📊 Train size: 293, Eval size: 74\n",
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m 🔧 Starting trial with config: {'batch_size': 1, 'lr': 9.263536712014789e-05, 'epochs': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/147 [00:00<?, ?it/s]\n",
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m /usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m   warnings.warn(\n",
      "  1%|          | 1/147 [00:00<00:56,  2.58it/s]\n",
      "  3%|▎         | 5/147 [00:00<00:12, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m {'loss': 5.5797, 'grad_norm': 20.786624908447266, 'learning_rate': 1.2521565460949866e-05, 'epoch': 0.03}\n",
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m {'loss': 5.7283, 'grad_norm': 20.491172790527344, 'learning_rate': 1.209274472598583e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/147 [00:00<00:07, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m {'loss': 5.9763, 'grad_norm': 24.713289260864258, 'learning_rate': 1.17496881380146e-05, 'epoch': 0.1}\n",
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m {'loss': 4.4338, 'grad_norm': inf, 'learning_rate': 9.231920545762179e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 16/147 [00:00<00:05, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m {'loss': 5.5181, 'grad_norm': 40.40081787109375, 'learning_rate': 1.1320867403050564e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 22/147 [00:01<00:05, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m 📊 Train size: 293, Eval size: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/293 [00:00<?, ?it/s]\n",
      " 33%|███▎      | 98/293 [00:04<00:09, 21.00it/s]\u001b[32m [repeated 51x across cluster]\u001b[0m\n",
      " 36%|███▌      | 105/293 [00:05<00:09, 20.78it/s]\u001b[32m [repeated 27x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m {'loss': 5.0977, 'grad_norm': 18.07731056213379, 'learning_rate': 3.17327343873387e-06, 'epoch': 0.78}\u001b[32m [repeated 38x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 136/147 [00:06<00:00, 21.17it/s]\n",
      " 95%|█████████▌| 140/147 [00:06<00:00, 21.15it/s]\n",
      " 97%|█████████▋| 142/147 [00:06<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m {'train_runtime': 7.1124, 'train_samples_per_second': 41.196, 'train_steps_per_second': 20.668, 'train_loss': 4.929324870206872, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 145/147 [00:07<00:00, 21.11it/s]\n",
      "100%|██████████| 147/147 [00:07<00:00, 20.67it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_with_tune_e7281_00000</td><td style=\"text-align: right;\">4.39772</td></tr>\n",
       "<tr><td>train_with_tune_e7281_00001</td><td style=\"text-align: right;\">4.02048</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m 📉 Eval results: {'eval_loss': 4.397721290588379, 'eval_runtime': 0.1617, 'eval_samples_per_second': 457.516, 'eval_steps_per_second': 61.827, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m /usr/local/lib/python3.12/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_with_tune pid=175877)\u001b[0m   _log_deprecation_warning(\n",
      " 85%|████████▍ | 248/293 [00:10<00:01, 34.55it/s]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      " 89%|████████▊ | 260/293 [00:10<00:00, 34.80it/s]\u001b[32m [repeated 29x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m {'loss': 2.978, 'grad_norm': 0.0, 'learning_rate': 1.233030483851798e-05, 'epoch': 0.89}\u001b[32m [repeated 38x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 270/293 [00:10<00:00, 34.32it/s]\n",
      " 93%|█████████▎| 272/293 [00:10<00:00, 34.24it/s]\n",
      " 94%|█████████▍| 276/293 [00:10<00:00, 34.46it/s]\n",
      " 96%|█████████▌| 280/293 [00:10<00:00, 34.56it/s]\n",
      " 97%|█████████▋| 284/293 [00:11<00:00, 34.38it/s]\n",
      " 98%|█████████▊| 288/293 [00:11<00:00, 34.18it/s]\n",
      " 99%|█████████▉| 290/293 [00:11<00:00, 34.18it/s]\n",
      "100%|██████████| 293/293 [00:11<00:00, 25.83it/s]\n",
      "2025-07-11 05:29:09,287\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_with_tune_2025-07-11_05-28-40' in 0.0040s.\n",
      "2025-07-11 05:29:09,291\tINFO tune.py:1041 -- Total run time: 28.91 seconds (28.89 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Best hyperparameters found: {'batch_size': 1, 'lr': 9.263536712014789e-05, 'epochs': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 116.93it/s]\n",
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m /usr/local/lib/python3.12/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_with_tune pid=175878)\u001b[0m 📉 Eval results: {'eval_loss': 4.020482540130615, 'eval_runtime': 0.0979, 'eval_samples_per_second': 755.745, 'eval_steps_per_second': 102.128, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def get_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model.to(device), tokenizer\n",
    "\n",
    "def load_tokenized_dataset(tokenizer, block_size=64):\n",
    "    raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "\n",
    "    def tokenize(example):\n",
    "        tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=block_size)\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy() \n",
    "        return tokens\n",
    "\n",
    "    tokenized = raw_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "    tokenized.set_format(\"torch\")\n",
    "    return tokenized\n",
    "\n",
    "def train_with_tune(config):\n",
    "    print(f\"🔧 Starting trial with config: {config}\")\n",
    "    try:\n",
    "        model, tokenizer = get_model_and_tokenizer()\n",
    "        dataset = load_tokenized_dataset(tokenizer)\n",
    "\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_dataset = dataset.select(range(train_size))\n",
    "        eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "        print(f\"📊 Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./output\",\n",
    "            per_device_train_batch_size=config[\"batch_size\"],\n",
    "            learning_rate=config[\"lr\"],\n",
    "            num_train_epochs=config[\"epochs\"],\n",
    "            logging_steps=5,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            fp16=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        # Use a data collator that supports masked LM tasks\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False  # GPT-style LM = causal, not masked\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        result = trainer.evaluate()\n",
    "        print(\"📉 Eval results:\", result)\n",
    "\n",
    "        eval_loss = result.get(\"eval_loss\", None)\n",
    "        if eval_loss is None:\n",
    "            print(\"⚠️ Warning: eval_loss missing. Reporting dummy loss = 9999.\")\n",
    "            eval_loss = 9999.0\n",
    "\n",
    "        #tune.report(loss=eval_loss)\n",
    "        train.report({\"loss\": eval_loss})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Trial crashed: {e}\")\n",
    "        train.report({\"loss\": 9999.0})\n",
    "\n",
    "# Define search space\n",
    "search_space = {\n",
    "    \"batch_size\": tune.choice([1, 2]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-4),\n",
    "    \"epochs\": tune.choice([1, 2]),\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler()\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_cpus=2)\n",
    "\n",
    "# Run tuning\n",
    "analysis = tune.run(\n",
    "    train_with_tune,\n",
    "    config=search_space,\n",
    "    num_samples=2,\n",
    "    scheduler=scheduler,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 0.5 if torch.cuda.is_available() else 0},\n",
    "    raise_on_failed_trial=False,\n",
    ")\n",
    "\n",
    "# Final result\n",
    "if analysis.best_config:\n",
    "    print(\"🎯 Best hyperparameters found:\", analysis.best_config)\n",
    "else:\n",
    "    print(\"⚠️ No successful trials. But now your model is ready to return loss correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61e16d-ce53-40c9-a35d-4ebe744b2b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
